# Machine-Learning-Notes
Draft of Machine Learning Notes for CS5339 Machine Learning at NUS. Note that this draft is still **incomplete** currently.

## Contents so far
### 1 Formulation
#### 1.1 Supervised Learning
1.1.1 Empirical Risk Minimization (ERM) <br>
1.1.2 Maximum Likelihood Estimation (MLE) <br>
1.1.3 Classification Loss Functions <br>
1.1.4 Maximum A Posteriori Estimation (MAPE) <br>
1.1.5 Bayesian Estimation <br>
#### 1.2 Unsupervised Learning
#### 1.3 Discriminative/Generative Models
1.3.1 Naive Bayes <br>
1.3.2 Linear Discriminant Analysis (LDA) <br>
1.3.3 Discriminative VS Generative Classifiers <br>
#### 1.4 Feature
1.4.1 Filter <br>
1.4.2 Wrapper <br>
1.4.3 Sparsity-Inducing Norms <br>
1.4.4 Common Feature Transformation Methods <br>
1.4.5 Feature Learning <br>
### 2 Representation
#### 2.1 Nearest Neighbor (NN)
#### 2.2 Decision Tree
#### 2.3 Linear Predictors
#### 2.4 Support Vector Machine
2.4.1 Margin and Hard SVM <br>
2.4.2 Representation Power of Linear Threshold Functions <br>
2.4.3 How many functions can a linear threshold function represent? <br>
#### 2.5 Linear combination of Functions
#### 2.6 Kernel Method
2.6.1 Hilbert Space <br>
2.6.2 Representer Theorem <br>
2.6.3 Characterizing Kernel Functions <br>
#### 2.7 Neural Networks
2.7.1 Representing All Boolean Functions <br>
2.7.2 Representing PARITY <br>
2.7.3 Approximating Real-valued Functions <br>
2.7.4 Summary of Representational Properties <br>
2.7.5 Single Hidden Layer NN In Practice <br>
#### 2.8 Matrix/Tensor Factorization
2.8.1 Principal Component Analysis (PCA) <br>
2.8.2 Latent Semantic Analysis (LSA) <br>
2.8.3 Representation as Linear Combination of Functions <br>
2.8.4 Collaborative Filtering <br>
#### 2.9 Ensemble Methods
2.9.1 Approximation with Square Loss
2.9.2 Maximum Norm Approximation
2.9.3 Using Ensemble for Classification
2.9.4 Large Margin Classifier and Weak Learning
2.9.5 Ensembles in Practice
#### 2.10 Deep Neural Network
2.10.1 VC Dimension of Neural Networks
2.10.2 Deep Vs Shallow
2.10.3 CNN
2.10.4 Deep Learning in Practice
#### 2.11 Embedding Knowledge/Algorithms
2.11.1 Properties of Convolutional Networks
2.11.2 RNN
2.11.3 Value Iteration Network
### 3 Estimation
#### 3.1 Finite Class
#### 3.2 PAC Learning
3.2.1 sample Complexity
3.2.2 Error Decomposition
#### 3.3 Uniform Convergence
#### 3.4 No Free Lunch
#### 3.5 Fundamental Theorem
3.5.1 Rademacher Complexity

## Credit
Most of the contents come from lecturer ([Prof Lee Wee Sun](https://www.comp.nus.edu.sg/~leews/))'s slides.
